{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef9a31aa-1afb-4dcb-82bb-4cf271aa9cfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": " Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".        ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 44\u001b[0m\n\u001b[1;32m     25\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m     26\u001b[0m         StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     27\u001b[0m         StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, LongType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m         StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfuel_consumed\u001b[39m\u001b[38;5;124m\"\u001b[39m, DoubleType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     31\u001b[0m     ])\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Consume data from the Kafka topic\u001b[39;00m\n\u001b[1;32m     35\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkafka\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkafka.bootstrap.servers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocalhost:9092\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubscribe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTruck-Data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstartingOffsets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mearliest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfailOnDataLoss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfalse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mincludeTimestamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mschema\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 44\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Define the schema for the data\u001b[39;00m\n\u001b[1;32m     47\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m     48\u001b[0m     StructField(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     49\u001b[0m     StructField(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     StructField(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuel_consumed\u001b[39m\u001b[38;5;124m'\u001b[39m, DoubleType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m ])\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py:469\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m:  Failed to find data source: kafka. Please deploy the application as per the deployment section of \"Structured Streaming + Kafka Integration Guide\".        "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a5c10-ed0b-4ad7-befb-ce7b9ebb1383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df375da-905a-4a17-8600-98e9bfe75107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, from_json, to_timestamp, avg\n",
    "from pyspark.sql.functions import avg\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import *\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Truck_Data_Stream\") \\\n",
    "    .config(\"spark.jars\", \"./../mysql_conn-JAR-FILE/mysql-connector-java-8.0.32.jar\") \\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# set log level to DEBUG\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "\n",
    "# Set partitionOverwriteMode to \"static\"\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"static\")\n",
    "\n",
    "logger = spark._jvm.org.apache.log4j\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "logger.LogManager.getLogger(\"org.apache.spark\").setLevel(logger.Level.DEBUG)\n",
    "logger.LogManager.getLogger(\"pyspark\").setLevel(logger.Level.DEBUG)\n",
    "logger.LogManager.getLogger(\"pyspark.sql.execution.streaming.StreamExecution\").setLevel(logger.Level.DEBUG)\n",
    "logger.LogManager.getLogger(\"pyspark.sql.execution.streaming.StreamExecutionRelation\").setLevel(logger.Level.DEBUG)\n",
    "\n",
    "\n",
    "# Define Kafka consumer options\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic_name = \"Truck-Data\"\n",
    "kafka_subscribe_type = \"subscribe\"\n",
    "kafka_consumer_group = \"test_group\"\n",
    "\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(kafka_subscribe_type, kafka_topic_name) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"failOnDataLoss\", \"false\") \\\n",
    "    .option(\"kafkaConsumer.pollTimeoutMs\", \"512\") \\\n",
    "    .option(\"kafkaConsumer.request.timeout.ms\", \"10000\") \\\n",
    "    .option(\"kafkaConsumer.session.timeout.ms\", \"30000\") \\\n",
    "    .option(\"kafkaConsumer.max.poll.records\", \"1000\") \\\n",
    "    .option(\"kafkaConsumer.auto.offset.reset\", \"latest\") \\\n",
    "    .option(\"kafkaConsumer.enable.auto.commit\", \"false\") \\\n",
    "    .option(\"kafkaConsumer.group.id\", kafka_consumer_group) \\\n",
    "    .load()\n",
    "\n",
    "\n",
    "# Define the schema for the messages received from Kafka\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"distance_covered\", DoubleType()),\n",
    "    StructField(\"engine_speed\", DoubleType()),\n",
    "    StructField(\"fuel_consumed\", DoubleType())    \n",
    "])\n",
    "\n",
    "# Convert Kafka messages into structured format using the defined schema\n",
    "df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"json\")) \\\n",
    "    .select(\"json.*\")\n",
    "\n",
    "\n",
    "\n",
    "df_select = df.select(\"timestamp\", \"distance_covered\", \"engine_speed\", \"fuel_consumed\")\n",
    "\n",
    "\n",
    "df2 = df.withColumn(\"speed_doubled\", col(\"engine_speed\") * 1.2)\n",
    "df3 = df.withColumn(\"distance_engine_ratio\", col(\"distance_covered\") / col(\"engine_speed\"))\n",
    "\n",
    "# Merge the two DataFrames\n",
    "df_combined = df2.union(df3)\n",
    "# Compute the ratio between distance_covered and engine_speed\n",
    "df_combined = df_combined.withColumn(\"distance_engine_ratio\", df_combined[\"distance_covered\"] / df_combined[\"engine_speed\"])\n",
    "# Select the desired columns\n",
    "df_combined_select = df_combined.select(\"speed_doubled\", \"distance_engine_ratio\")\n",
    "\n",
    "\n",
    "\n",
    "df_filtered = df.filter(col(\"engine_speed\") > 80).alias(\"filtered\")\n",
    "df_select = df.select(\"timestamp\", \"distance_covered\", \"engine_speed\").alias(\"selected\")\n",
    "df_combined_2 = df_filtered.join(df_select, \"timestamp\", \"inner\").select(\"timestamp\", \"filtered.distance_covered\", \"selected.engine_speed\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print the received messages to a file\n",
    "query = df2.select(\"speed_doubled\").coalesce(1).writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"./../spark_job_outputs/job_output_formats_folder/df2_output.csv\") \\\n",
    "    .option(\"checkpointLocation\", \"./../spark_job_outputs/df2_checkpoint_folder/\") \\\n",
    "    .option(\"maxRecordsPerFile\", 100000) \\\n",
    "    .trigger(processingTime=\"15 seconds\") \\\n",
    "    .start() \n",
    "\n",
    "\n",
    "# Print the received messages to the console\n",
    "query = df2.coalesce(1).writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"./../spark_job_outputs/job_output_formats_folder/output.csv\") \\\n",
    "    .option(\"checkpointLocation\", \"./../spark_job_outputs/checkpoint_folder/\") \\\n",
    "    .option(\"maxRecordsPerFile\", 100000) \\\n",
    "    .trigger(processingTime=\"15 seconds\") \\\n",
    "    .start() \n",
    "\n",
    "# Print the received messages to a file\n",
    "query = df3.select(\"distance_engine_ratio\").coalesce(1).writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"./../spark_job_outputs/job_output_formats_folder/df3_output.csv\") \\\n",
    "    .option(\"checkpointLocation\", \"./../spark_job_outputs/df3_checkpoint_folder\") \\\n",
    "    .option(\"maxRecordsPerFile\", 100000) \\\n",
    "    .trigger(processingTime=\"15 seconds\") \\\n",
    "    .start() \n",
    "\n",
    "# Print the received messages to a file\n",
    "query = df_combined_select.coalesce(1).writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"./../spark_job_outputs/job_output_formats_folder/df_combined_output.csv\") \\\n",
    "    .option(\"checkpointLocation\", \"./../spark_job_outputs/df_combined_checkpoint_folder\") \\\n",
    "    .option(\"maxRecordsPerFile\", 100000) \\\n",
    "    .trigger(processingTime=\"15 seconds\") \\\n",
    "    .start() \n",
    "\n",
    "\n",
    "# Print the received messages to a file\n",
    "query = df_combined_2.coalesce(1).writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"path\", \"./../spark_job_outputs/job_output_formats_folder/df_combined_2_output.csv\") \\\n",
    "    .option(\"checkpointLocation\", \"./../spark_job_outputs/df_combined_2_checkpoint_folder\") \\\n",
    "    .option(\"maxRecordsPerFile\", 100000) \\\n",
    "    .trigger(processingTime=\"15 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "\n",
    "# Write to MySQL Table\n",
    "def write_to_mysql(df, epoch_id):\n",
    "    df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"driver\",\"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .option(\"url\", \"jdbc:mysql://MySQL_Container:3306/KAFKA_DB\") \\\n",
    "        .option(\"path\", \"./../spark_job_outputs/job_output_formats_folder/df_output.csv\") \\\n",
    "        .option(\"dbtable\", \"SPARK_TABLE_TRANSFORMED\") \\\n",
    "        .option(\"mode\", \"append\") \\\n",
    "        .option(\"user\", \"root\") \\\n",
    "        .option(\"password\", \"root\") \\\n",
    "        .save()\n",
    "\n",
    "df.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"./../spark_job_outputs/df_checkpoint_folder\") \\\n",
    "    .foreachBatch(write_to_mysql) \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Here we start the streaming SparkSQL queries which we wrote above. \n",
    "'''\n",
    "# Start the streaming query and wait for it to finish\n",
    "query.awaitTermination()\n",
    "\n",
    "while query.isActive:\n",
    "    try:\n",
    "        print(query.lastProgress)\n",
    "    except KeyboardInterrupt:\n",
    "        query.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
