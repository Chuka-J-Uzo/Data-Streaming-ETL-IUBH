
# install MySQL according to the linux debian currently running on codespaces (I got the image from https://hub.docker.com/_/mysql)
$ docker pull mysql:8.0.32-debian

# Starting our MySQL instance (find this command and details from https://hub.docker.com/_/mysql):
docker run --name MySQL_Container -e MYSQL_ROOT_PASSWORD=root -p 3306:3306 -d mysql:8.0.32-debian

# above we called our container "MySQL_Container"


# Now we install phpMyAdmin - A web interface for MySQL
$ docker pull phpmyadmin:latest

# After pully phpMyAdmin, next, we need to run a MySQL server in Docker, and then the phpMyAdmin image needs to be linked to the running database container like this:
$ docker run --name phpmyadmin -d --link MySQL_Container:db --restart always  -p 8085:80 phpmyadmin
# Notice that our running container name "MySQL_Container" will then be linked to our phpMyAdmin image by adding "--link MySQL_Container:db" be sure to put the db at the end.

# activate your MySQL shell by right-clicking against the MySQL_Container which happens to be our MySQL server.
   Then you'll see this prompt waiting for us to login to MySQL ---> root@a20370fbdbfc:/#  
   Now type this into the shell to login. We use 'root' as username -u and 'root' as -p password. 
   the command will look like this ---->    mysql -u root  -p
    
   If that doesn't work, do this ------>     mysql --user=root --password=root 


   - Then we create a database to collect Kafka's producer messages. we will call the database "KAFKA_DB"
      mysql> CREATE DATABASE KAFKA_DB;

   - Then create a table inside this databse.
     To do this, we use the "use" command on the created DB as follows:
     mysql> use KAFKA_DB;

     Then we create a Table using the schema or column names in the data we are trying to ingest. Our table is called "INGESTED_TABLE_1"
     CREATE TABLE INGESTED_TABLE_1 (time float, seconds_elapsed float,  qz float, qy float, qx float, qw float, roll float, pitch float, yaw float);
     
CREATE TABLE TRUCK_PARAMETER_MAP (
         `id` INT AUTO_INCREMENT PRIMARY KEY,
         `timestamp` DATETIME,
         `distance_covered` FLOAT,
         `engine_speed` FLOAT,
         `fuel_consumed` FLOAT
     );


CREATE TABLE TRUCK_PARAMETER_MAP (
    `current_time` DATETIME,
    `road_elevation` FLOAT,
    `Engine Speed` FLOAT,
    `Coolant Temp` FLOAT,
    `Oil Pressure` FLOAT,
    `Oil Temp` FLOAT,
    `Throttle Position` FLOAT,
    `Fuel Pressure` FLOAT,
    `Mass Airflow Rate` FLOAT,
    `Intake Air Temp` FLOAT,
    `Engine Load` FLOAT,
    `Battery Voltage` FLOAT,
    `Acceleration` FLOAT,
    `Fuel Level` FLOAT,
    `Exhaust Gas Temp` FLOAT,
    `Engine Hours` FLOAT,
    `Tyre Pressure` FLOAT,
    PRIMARY KEY (`current_time`)
);

CREATE TABLE COMMODITIES_PRICES (
    `id` INT AUTO_INCREMENT PRIMARY KEY, \
    `symbol` VARCHAR(10) NOT NULL, \
    `name` VARCHAR(100) NOT NULL, \
    `last_price` DECIMAL(10, 2), \
    `market_time` DATETIME, \
    `price_change` DECIMAL(10, 2), \
    `percent_change` DECIMAL(5, 2), \
    `volume` BIGINT
);






# FOR THOSE USING LOCAL MACHINES, After running MySQL on docker, and you are trying to enter mysql prompt, but you get this error, ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2)

Just use the command below....replace with your container name and user name:
sudo docker exec -it <container_name> mysql -u <user_name> -p
                             OR 
sudo docker exec -it MySQL_Container mysql -u root -p
                             OR
mysql --user=root --password=root


If Kafka fails after running it, the docker compose setting for kafka
were likely wrong, use this instead:


docker run -d \
    --net=host \
    --name=broker \
    -e KAFKA_ZOOKEEPER_CONNECT=localhost:2181 \
    -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:29092 \
    -e KAFKA_BROKER_ID=1 \
    -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \
    confluentinc/cp-kafka:7.3.1



# Create a Kafka topic (right-click against the running Kafka broker or kafka server). We call it "data-inflow-1"
$ ./kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic data-inflow-1

OR

kafka-topics.sh --create --topic Truck-Data --partitions 3 --replication-factor 1 --bootstrap-server localhost:9092 --config segment.ms=10000 --config retention.ms=3600000

# Install confluent kafka python client
pip install confluent-kafka

# Do the same for mysql-connector to enable python talk to our MySQL DB
pip install mysql-connector-python


# Now install Pyspark 

This application will create a jupyter notebook ready with Pyspark and Spark 3.1.2 with Hadoop 3.2
- Use this link to install pyspark on docker as above: https://ruslanmv.com/blog/Docker-Container-with-Pyspark-and-Jupyter-and-Elyra
- After you have built the image with: docker build --rm -t ruslanmv/pyspark-notebook .
- Use this to run the container: docker run  --name pyspark-notebook  -it -p 8888:8888  -v /home/blackjack/Data-Streaming-ETL-IUBH-main/Data-Streaming-ETL-IUBH:/home/jovyan/work  -d ruslanmv/pyspark-notebook
- Now check for jupyter URL(with token) in the pyspark container's logs. right-click on it an select "view logs".


# How to start Kafka broker with a large memory allocation
docker run -d -p 9092:9092 --restart=always  --memory=4g --memory-swap=4g confluentinc/cp-server:7.3.1

kafka-topics --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic data-inflow-1

#Print out my table to mysql terminal
SELECT * FROM table_name;




# INSTALL APACHE SUPERSET

#1. Firstly, Docker pull Apache superset
$ docker pull apache/superset:latest

#2. Start a superset instance on port 8080

$ sudo docker run -d -p 8090:8088 --name superset apache/superset

# i changed 8080:8088 port to 8090:8088 so it doesn't class with an existing container running on 80:80


#3.Initialize a local Superset Instance

With my local superset container already running...

Then I Setup my local admin account:
$ sudo docker exec -it superset superset fab create-admin \
              --username admin \
              --firstname Superset \
              --lastname Admin \
              --email admin@superset.com \
              --password admin

#4. Now we should migrate local DB to latest

$ sudo docker exec -it superset superset db upgrade

#5. Load Examples

$ sudo docker exec -it superset superset load_examples

#6. Setup roles

$ sudo docker exec -it superset superset init

#7. Now, Login and take a look -- navigate to http://localhost:8090/login/ 
     use user: admin
     use password: admin




# Installing Apache druid:

$ docker pull apache/druid:25.0.0


``` SELECT * FROM INGEST_PANDAS_PROCESSED_1 LIMIT 900000;   ```





docker pull ubuntu/kafka:3.1-22.04_beta
https://hub.docker.com/r/ubuntu/kafka
docker run -d --name kafka-container -e TZ=UTC -p 9092:9092 -e ZOOKEEPER_HOST=host.docker.internal ubuntu/kafka:3.1-22.04_beta


docker pull ubuntu/zookeeper:3.1-22.04_beta
https://hub.docker.com/r/ubuntu/zookeeper
docker run -d --name zookeeper-container -e TZ=UTC -p 2181:2181 ubuntu/zookeeper:3.1-22.04_beta

Expose the Kafka port to the host network. You can do this by adding the --network host option to the docker run command:
docker run -d --name kafka-container --network host -e TZ=UTC -e KAFKA_ADVERTISED_HOST_NAME=host.docker.internal -e KAFKA_ADVERTISED_PORT=9092 -e ZOOKEEPER_HOST=host.docker.internal ubuntu/kafka:3.1-22.04_beta
Note that in this case, you do not need to publish the Kafka port with -p since the container is using the host network. Also, make sure that any firewall or security groups are not blocking traffic to the Kafka port.



SPARK SCHEMA CREATION

schema = StructType([
        StructField("id", IntegerType(), True),
        StructField("timestamp", LongType(), True),
        StructField("distance_covered", DoubleType(), True),
        StructField("engine_speed", DoubleType(), True),
        StructField("fuel_consumed", DoubleType(), True),
    ])

